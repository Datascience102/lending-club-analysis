---
title: "LendingClubAnalysis"
author: "Steve Isaacs, Kailing See, Sally Guo, Cristian Benavides"
date: "1/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

install.packages("ggthemr")
install.packages("knitr")
install.packages("rpart")
install.packages("corrplot")

library("ggthemr")
library("knitr")
library("rpart")
library("corrplot")
```

Business Problem: Assessing credit worthiness of customers in Lending Club and their probability of defaulting.

Data set: https://www.kaggle.com/wendykan/lending-club-loan-data

### 1.Introduction to the Business Problem

Lending Club is the world’s largest online marketplace connecting borrowers and investors.They operate at a lower cost than traditional bank lending programs and pass the savings on to borrowers in the form of lower rates and to investors in the form of solid returns.
Source: https://www.lendingclub.com/

This project aims to analyse Lending Club's issued loans over a 9 year period (2007-2015) and identify early indicators that could predict a customer's probability of defaulting. The insight obtained could help to drive differentiated credit approval processes for different customer segments (e.g faster approval time, minimal security for low risk customers).

### 2.Process to solve the Business Problem

We would suggest the following process:      
1. Understand Lending Club Business Model & Data Set    
2. Clean the data, identify relevant attributes/columns & separate training and testing dataset    
3. Generate Hypothesis    
4. Apply Dimensionality Reduction on Hypothesis     
5. Apply Segmentation & Classification Tree on Training Dataset    
6. Apply Algorithm on Testing Dataset (Iterate if required)    
7. Capture insights & Summarise    

**OBoulant comments :**
If you want to "identify early indicators that could predict a customer’s probability of defaulting", you have to identify into the dataset, a column that would "represent" the "customer’s probability of defaulting". 
--> Do you have an idea of which column within the data could play this role? ProjectData$loan_status

### 3.Key Summary of Data

```{r eval = FALSE, echo=TRUE, comment=NA, warning=FALSE, message=FALSE,results='markup'}
ProjectData<- read.csv(file = "Data/loan.csv", header = TRUE, sep=",", stringsAsFactors = FALSE, nrows=10000)
# OBoulant : i added the "stringsAsFactors = FALSE", because I want to look at the variables represented by character strings
# SIsaacs: I added "nrows = 10000" to work with a smaller more manageable data set
# print(colnames(ProjectData))
# summary(ProjectData)

ProjectDataClean <- ProjectData$id
ProjectDataClean <- cbind(ProjectDataClean, ProjectData$loan_status, ProjectData$loan_amnt, ProjectData$term, ProjectData$annual_inc, ProjectData$emp_length, ProjectData$pub_rec, ProjectData$open_acc, ProjectData$delinq_2yrs, ProjectData$grade, ProjectData$int_rate)
colnames(ProjectDataClean) <- c("ID", "LoanStatus", "LoanAmount", "LoanTerm", "AnnualIncome", "EmploymentLength", "PublicRecords", "NumberCreditLines", "DelinquencyPrior2Years", "LoanGrade", "InterestRate")

#Convert the data to numbers
ProjectDataClean[,"LoanStatus"] <- replace(ProjectDataClean[,"LoanStatus"], ProjectDataClean[,"LoanStatus"]=="Charged Off", 1)
ProjectDataClean[,"LoanStatus"] <- replace(ProjectDataClean[,"LoanStatus"], ProjectDataClean[,"LoanStatus"]=="Current", 0)
ProjectDataClean[,"LoanStatus"] <- replace(ProjectDataClean[,"LoanStatus"], ProjectDataClean[,"LoanStatus"]=="Fully Paid", 0)

ProjectDataClean[,"LoanTerm"] <- replace(ProjectDataClean[,"LoanTerm"], ProjectDataClean[,"LoanTerm"]==" 36 months", 36)
ProjectDataClean[,"LoanTerm"] <- replace(ProjectDataClean[,"LoanTerm"], ProjectDataClean[,"LoanTerm"]==" 60 months", 60)

ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="< 1 year", 0)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="1 year", 1)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="2 years", 2)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="3 years", 3)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="4 years", 4)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="5 years", 5)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="6 years", 6)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="7 years", 7)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="8 years", 8)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="9 years", 9)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="10+ years",10)
ProjectDataClean[,"EmploymentLength"] <- replace(ProjectDataClean[,"EmploymentLength"], ProjectDataClean[,"EmploymentLength"]=="n/a",NA)

ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="A",1)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="B",2)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="C",3)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="D",4)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="E",5)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="F",6)
ProjectDataClean[,"LoanGrade"] <- replace(ProjectDataClean[,"LoanGrade"], ProjectDataClean[,"LoanGrade"]=="G",7)

#convert to numeric
ProjectDataNumeric<-matrix(as.numeric(unlist(ProjectDataClean)),nrow=nrow(ProjectDataClean))
colnames(ProjectDataNumeric) <- c("ID", "LoanStatus", "LoanAmount", "LoanTerm", "AnnualIncome", "EmploymentLength", "PublicRecords", "NumberCreditLines", "DelinquencyPrior2Years", "LoanGrade", "InterestRate")

#remove NAs
ProjectDataNumeric<-na.omit(ProjectDataNumeric)

#correlation matrix to explore data
acor <- cor(ProjectDataMatrix)
corrplot(acor, type="lower", tl.srt=45)

#scale data
ProjectDataScaled<-ProjectDataNumeric
ProjectDataScaled[,3:6]<-apply(ProjectDataNumeric[,3:6],2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
ProjectDataScaled[,8:11]<-apply(ProjectDataNumeric[,8:11],2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})

#Consider loan grade, subgrade and interest rate separately too    

#SIsaacs Questions: how to scale data?                          
                          
# OBoulant comments :
######################

# You should decide what you want to do with categorical variables 
##################################################################

## 1 - term
# Should term be represented as character strings ("36 months" and "60 months") or by 36 and 60 ? 
unique(ProjectData$term)
ProjectData$term <- as.factor(ProjectData$term)
## 2 - Grades 
unique(ProjectData$grade)
ProjectData$grade <- as.factor(ProjectData$grade)
## 3 - sub_grade
unique(ProjectData$sub_grade)
ProjectData$sub_grade <- as.factor(ProjectData$sub_grade)
## 4 - emp_title
unique(ProjectData$emp_title)
head(table(ProjectData$emp_title))
sum(ProjectData$emp_title == '') # Many blanks !
# This field should be re-processed
## 5 - emp_length
unique(ProjectData$emp_length)
ProjectData$emp_length <- as.factor(ProjectData$emp_length)
## 6 - home_ownership
unique(ProjectData$home_ownership)
ProjectData$home_ownership <- as.factor(ProjectData$home_ownership)
## 7 - home_ownership
unique(ProjectData$verification_status)
ProjectData$verification_status <- as.factor(ProjectData$verification_status)
## 8 - issue_d
unique(ProjectData$issue_d)
ProjectData$issue_d <- as.factor(ProjectData$issue_d)
# Maybe should be separated in issue_year and issue_month
## 9 - loan_status
unique(ProjectData$loan_status)
ProjectData$loan_status <- as.factor(ProjectData$loan_status)
## 10 - pymnt_plan
unique(ProjectData$pymnt_plan)
ProjectData$pymnt_plan <- as.factor(ProjectData$pymnt_plan)
## 11 - url
unique(ProjectData$url) # Too many modalities, keep it in character strings - Not to be used !
## 12 - desc
unique(ProjectData$desc) # Too many modalities, keep it in character strings - Not to be used !
## 13 - purpose 
unique(ProjectData$purpose)
ProjectData$purpose <- as.factor(ProjectData$purpose)
## 14 - title
unique(ProjectData$title)
# Either should be re-processed or not to be used ! Or do a word count, i don't know
## 15 - zip_code
unique(ProjectData$zip_code)
ProjectData$zip_code <- as.factor(ProjectData$zip_code)
## 16 - addr_state
unique(ProjectData$addr_state)
ProjectData$addr_state <- as.factor(ProjectData$addr_state)
## 17 - earliest_cr_line
unique(ProjectData$earliest_cr_line)
# Some missing Data
# Maybe should be separated into earliest_cr_line_month and earliest_cr_line_year
ProjectData$earliest_cr_line <- as.factor(ProjectData$earliest_cr_line)
## 18 - initial_list_status
unique(ProjectData$initial_list_status)
ProjectData$initial_list_status <- as.factor(ProjectData$initial_list_status)
## 19 - last_pymnt_d
unique(ProjectData$last_pymnt_d)
ProjectData$last_pymnt_d <- as.factor(ProjectData$last_pymnt_d)
# Some missing Data
# Maybe should be separated into last_pymnt_month and last_pymnt_year
## 20 - next_pymnt_d
unique(ProjectData$next_pymnt_d)
# Some missing Data
# Maybe should be separated into next_pymnt_month and next_pymnt_year
ProjectData$next_pymnt_d <- as.factor(ProjectData$next_pymnt_d)
# 21 - last_credit_pull_d
unique(ProjectData$last_credit_pull_d)
# Some missing Data
# Maybe should be separated into last_credit_pull_month and last_credit_pull_year
ProjectData$last_credit_pull_d <- as.factor(ProjectData$last_credit_pull_d)
# 22 - application_type
unique(ProjectData$application_type)
ProjectData$application_type <- as.factor(ProjectData$application_type)
# 23 - verification_status_joint
unique(ProjectData$verification_status_joint)
ProjectData$verification_status_joint <- as.factor(ProjectData$verification_status_joint)

# Result Dataset
summary(ProjectData)

```

### 4.Dimensionality Reduction

```{r eval = FALSE, echo=TRUE, comment=NA, warning=FALSE, message=FALSE,results='markup'}

dependent_variable<-2
independent_variables<-c(3:11)

#profit/loss values
#actual default, predict default
actual_1_predict_1 <- 50
#actual default, predict repayment
actual_1_predict_0 <- -100
#actual repayment, predict default
actual_0_predict_1 <- -50
#actual repayment, predict repayment
actual_0_predict_0 <- 100

Probability_Threshold <- 0.5
estimation_data_percent <- 80
validation_data_percent <- 10
random_sampling = 0

CART_cp <- 0.01
min_segment <- 100
max_data_report <- 10

Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix)<- c("Predict 1", "Predict 0")
rownames(Profit_Matrix) <- c("Actual 1", "Actual 0")
test_data_percent = 100-estimation_data_percent-validation_data_percent
CART_control = rpart.control(cp = CART_cp)

if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectDataScaled),floor(estimation_data_percent*nrow(ProjectDataScaled)/100))
  non_estimation_data = setdiff(1:nrow(ProjectDataScaled),estimation_data_ids)
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectDataScaled)/100)
    non_estimation_data = setdiff(1:nrow(ProjectDataScaled),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectDataScaled), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectDataScaled[estimation_data_ids,]
validation_data=ProjectDataScaled[validation_data_ids,]
test_data=ProjectDataScaled[test_data_ids,]

```

```{r eval = TRUE, echo=TRUE, comment=NA, warning=FALSE, message=FALSE,results='markup'}
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```

```{r eval = TRUE, echo=TRUE, comment=NA, warning=FALSE, message=FALSE,results='markup'}
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```

```{r}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,independent_variables]),2))
```

```{r}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,independent_variables]),2))
```

```{r, fig.height=4.5}
DVvalues = unique(estimation_data[,dependent_variable])
x0 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[1]),independent_variables]
x1 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[2]),independent_variables]
colnames(x0) <- 1:ncol(x0)
colnames(x1) <- 1:ncol(x1)

swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x0))))
ggplot(melt(cbind.data.frame(n=1:nrow(x0), x0), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x0)))
set_swatch(swatch.default)
```

```{r, fig.height=4.5}
swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x1))))
ggplot(melt(cbind.data.frame(n=1:nrow(x1), x1), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x1)))
set_swatch(swatch.default)
```

```{r}
# just name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

rpart.plot(CART_tree)
```

```{r}
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = 0.005))
rpart.plot(CART_tree_large)
```

```{r}
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]

estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table_large)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
```

```{r}
formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~")

logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)
log_coefficients <- round(summary(logreg_solution)$coefficients,1)

knitr::kable(round(log_coefficients,2))
```

```{r}
# Let's get the probabilities for the 3 types of data again
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
```

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))

tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)

large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)

Importance_table <- cbind(tree_importance_final,large_tree_importance_final, log_importance)
colnames(Importance_table) <- c("CART 1", "CART 2", "Logistic Regr.")
rownames(Importance_table) <- rownames(log_importance)
## printing the result in a clean-slate table
knitr::kable(round(Importance_table,2))
```

### 5.Clustering and Segmentation

### 6. Results 

>>>>>>> 7664b6275553d5ba40165c41ff18eb644596ceb4
